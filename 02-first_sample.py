# -*- coding: utf-8 -*-
"""First sample

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ec-ZO6dh7XgQ706Og_MEsgfevPbSby8s

Clean
"""

# Step 1: Install required packages
!pip install -q transformers datasets torchaudio jiwer openai-whisper

# Step 2: Upload your audio file
from google.colab import files
uploaded = files.upload()

import os
audio_path = list(uploaded.keys())[0]
print("\n‚úÖ Audio file uploaded:", audio_path)

# Step 3: Transcription with Whisper
import whisper

whisper_model = whisper.load_model("base")
whisper_result = whisper_model.transcribe(audio_path)
whisper_text = whisper_result["text"]
print("\nüó£Ô∏è Whisper Transcription:\n", whisper_text)

# Step 4: Transcription with Wav2Vec2
import torchaudio
from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC
import torch

# Load model and processor
processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base-960h")
model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h")

# Load and resample audio
speech_array, sampling_rate = torchaudio.load(audio_path)
resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=16000)
resampled = resampler(speech_array).squeeze().numpy()

# Tokenize and predict
input_values = processor(resampled, return_tensors="pt", sampling_rate=16000).input_values
with torch.no_grad():
    logits = model(input_values).logits
predicted_ids = torch.argmax(logits, dim=-1)
wav2vec_text = processor.decode(predicted_ids[0])

print("\nüéôÔ∏è Wav2Vec2 Transcription:\n", wav2vec_text)

# Step 5: Compute and display WER
from jiwer import wer

ground_truth = input("\n‚úçÔ∏è Enter the ground truth transcription:\n")

wer_whisper = wer(ground_truth.lower(), whisper_text.lower())
wer_wav2vec = wer(ground_truth.lower(), wav2vec_text.lower())

print("\nüìä Evaluation Results:")
print(f"üîπ Word Error Rate (Whisper): {wer_whisper:.2f}")
print(f"üîπ Word Error Rate (Wav2Vec2): {wer_wav2vec:.2f}")

"""## **Noisy**"""

# Step 1: Install required packages
!pip install -q transformers datasets torchaudio jiwer openai-whisper

# Step 2: Upload your audio file
from google.colab import files
uploaded = files.upload()

import os
audio_path = list(uploaded.keys())[0]
print("\n‚úÖ Audio file uploaded:", audio_path)

# Step 3: Transcription with Whisper
import whisper

whisper_model = whisper.load_model("base")
whisper_result = whisper_model.transcribe(audio_path)
whisper_text = whisper_result["text"]
print("\nüó£Ô∏è Whisper Transcription:\n", whisper_text)

# Step 4: Transcription with Wav2Vec2
import torchaudio
from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC
import torch

# Load model and processor
processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base-960h")
model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h")

# Load and resample audio
speech_array, sampling_rate = torchaudio.load(audio_path)
resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=16000)
resampled = resampler(speech_array).squeeze().numpy()

# Tokenize and predict
input_values = processor(resampled, return_tensors="pt", sampling_rate=16000).input_values
with torch.no_grad():
    logits = model(input_values).logits
predicted_ids = torch.argmax(logits, dim=-1)
wav2vec_text = processor.decode(predicted_ids[0])

print("\nüéôÔ∏è Wav2Vec2 Transcription:\n", wav2vec_text)

# Step 5: Compute and display WER
from jiwer import wer

ground_truth = input("\n‚úçÔ∏è Enter the ground truth transcription:\n")

wer_whisper = wer(ground_truth.lower(), whisper_text.lower())
wer_wav2vec = wer(ground_truth.lower(), wav2vec_text.lower())

print("\nüìä Evaluation Results:")
print(f"üîπ Word Error Rate (Whisper): {wer_whisper:.2f}")
print(f"üîπ Word Error Rate (Wav2Vec2): {wer_wav2vec:.2f}")

'''ASR Model Performance Analysis

Whisper and Wav2Vec performance was checked on clean and noisy versions of the **same audio**:
- **Whisper** WER went from **0.09 to 0.13**
- **Wav2Vec** WER went from **0.35 to 0.78**'''
