# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FPMUbA5vb20nu5rXSOZUZrM7WEzfenIv
"""

!pip install -q openai-whisper torchaudio jiwer requests tqdm

import torch
import whisper
import torchaudio
from torchaudio.datasets import LIBRISPEECH
from jiwer import wer
import re
import requests
from tqdm import tqdm
from getpass import getpass

# Configuration
WHISPER_MODEL = "medium"  # Consider "small" or "medium" for better accuracy
NUM_SAMPLES = 20
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
MIN_WER_IMPROVEMENT = 0.15  # Require 15% WER reduction to accept correction

# Initialize Whisper
print(f"Loading Whisper ({WHISPER_MODEL}) on {DEVICE}...")
whisper_model = whisper.load_model(WHISPER_MODEL).to(DEVICE)

# Mistral API Setup
MISTRAL_API_KEY = getpass("Enter Mistral API key: ")
headers = {
    "Authorization": f"Bearer {MISTRAL_API_KEY}",
    "Content-Type": "application/json"
}

def normalize_text(text):
    """Standardize text for accurate comparison"""
    text = text.lower()
    return re.sub(r'[^\w\s]', '', text)

def rule_based_corrector(text):
    """Fix only verified common Whisper errors"""
    corrections = {
        r"\bdus daar\b": "so there",
        r"\btady kyrness\b": "tardy kindness",
        r"\bopaa\b": "papa",
        r"\bchoppy\b": "shoppy",
        r"\blift\b": "left"
    }
    for pattern, replacement in corrections.items():
        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
    return text

def validate_correction(original, corrected, reference):
    """Only accept corrections with significant WER improvement"""
    orig_wer = wer(normalize_text(reference), normalize_text(original))
    new_wer = wer(normalize_text(reference), normalize_text(corrected))
    return corrected if (orig_wer - new_wer) >= MIN_WER_IMPROVEMENT else original

def format_comparison(sample_id, ref, whisper_txt, mixtral_txt):
    """Color-coded display of results"""
    print(f"\n\033[1mSample {sample_id}:\033[0m")
    print(f"Reference: {ref}")
    print(f"\033[94mWhisper:  {whisper_txt}\033[0m")
    print(f"\033[92mMixtral:  {mixtral_txt}\033[0m")
    print("-" * 80)

# Main processing loop
dataset = LIBRISPEECH("./", url="test-other", download=True)
results = []

for i in tqdm(range(min(NUM_SAMPLES, len(dataset)))):
    # Load data
    waveform, _, reference, _, _, _ = dataset[i]
    file_path = f"temp_{i}.wav"
    torchaudio.save(file_path, waveform, 16000)

    # Transcribe with Whisper
    whisper_text = whisper_model.transcribe(file_path, fp16=(DEVICE=="cuda"))["text"].strip()
    base_text = rule_based_corrector(whisper_text)

    # Conditional Mixtral correction
    final_text = base_text
    mixtral_text = base_text

    if wer(normalize_text(reference), normalize_text(base_text)) > 0.3:
        try:
            response = requests.post(
                "https://api.mistral.ai/v1/chat/completions",
                headers=headers,
                json={
                    "model": "mistral-small",
                    "messages": [{
                        "role": "user",
                        "content": f"""
                        Fix ONLY these errors: {list(corrections.values())}
                        Preserve: names, numbers, sentence structure
                        Transcript: "{base_text}"
                        \



                        fhfcvgvb












                        .,k
                        """
                    }],
                    "temperature": 0.0,
                    "max_tokens": 200
                },
                timeout=15
            )
            if response.status_code == 200:
                mixtral_text = response.json()["choices"][0]["message"]["content"].strip()
                final_text = validate_correction(base_text, mixtral_text, reference)
        except Exception as e:
            pass

    # Store results
    results.append({
        "sample": i,
        "wer_whisper": wer(normalize_text(reference), normalize_text(whisper_text)),
        "wer_final": wer(normalize_text(reference), normalize_text(final_text)),
        "reference": reference,
        "whisper": whisper_text,
        "final": final_text
    })

    # Display comparison
    format_comparison(i, reference, whisper_text, final_text)

# Statistics
avg_whisper_wer = sum(r["wer_whisper"] for r in results) / len(results)
avg_final_wer = sum(r["wer_final"] for r in results) / len(results)
improvement = (avg_whisper_wer - avg_final_wer) / avg_whisper_wer * 100

print("\n\033[1mFinal Report:\033[0m")
print(f"Average WER (Original): {avg_whisper_wer*100:.1f}%")
print(f"Average WER (Corrected): {avg_final_wer*100:.1f}%")
print(f"Effective Improvement: {improvement:.1f}%")